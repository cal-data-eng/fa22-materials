{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"proj4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Mongo \n",
    "\n",
    "## Due Date: Friday 11/18, 11:59 PM\n",
    "\n",
    "In this project, we will be investigating how different database systems handle semi-structured JSON data. In particular, we will be placing emphasis on the use of MongoDB: a database system that stores data in a construct known as documents. These documents are very similar to the JSON objects we've explored in lecture with a few differences in representation and indexing that we will explore in the following questions. In this project, we will be working with the Yelp Academic Dataset which contains a dataset of `businesses`, `reviews`, and `users`. Due to the limitations of JupyterHub and the Mongo instances we are working with, `reviews` and `users` are truncated to 7500 reviews and 1000 users. We will be using the full `businesses` dataset, however.\n",
    "\n",
    "Throughout the course of this project, you should understand what Mongo can (and cannot) do with regards to its documents as a NoSQL datastore and compare and contrast this to other data representation formats such as the relational model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics & Scoring Breakdown\n",
    "\n",
    "For Data 101 students, this project is worth 15% of your grade. For Info 258 students, this project is worth 12% of your grade.\n",
    "\n",
    "Each coding question has **both public tests and hidden tests**. Roughly 50% of your coding grade will be made up of your score on the public tests released to you, while the remaining 50% will be made up of unreleased hidden tests. Free-response questions will be manually graded.\n",
    "\n",
    "This is an **individual project**. However, you’re welcome to collaborate with any other student in the class as long as it’s within the academic honesty guidelines.\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "1a\t| 1\n",
    "1b  | 1\n",
    "1c\t| 2\n",
    "1d\t| 1\n",
    "1e\t| 2\n",
    "1f  | 1\n",
    "2a\t| 2\n",
    "2b\t| 1\n",
    "2c  | 1\n",
    "2d  | 2\n",
    "3a\t| 1\n",
    "3b\t| 1\n",
    "3c\t| 1\n",
    "3d  | 1\n",
    "3e  | 1\n",
    "3f  | 3\n",
    "4a\t| 1\n",
    "4b\t| 2\n",
    "4c\t| 3\n",
    "4d  | 1\n",
    "**Total** | 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Up Mongo\n",
    "We will be using Pymongo, a Python wrapper for MongoDB, for this project. Every student should have access to their own MongoDB instance, running on the localhost of your Datahub server. After running the following cell, for the rest of the project, you can use the Python variables business, review, and user to access the corresponding collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bson\n",
    "from bson.objectid import ObjectId\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "from pymongo import TEXT\n",
    "\n",
    "myclient = pymongo.MongoClient(\"mongodb://localhost\")\n",
    "mydb = myclient[\"yelp\"]\n",
    "business = mydb[\"business\"]\n",
    "review = mydb[\"review\"]\n",
    "user = mydb[\"user\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "**PLEASE READ:** Please avoid printing too much debugging query output - it may crash your Jupyter Hub if your file size becomes too large! It's recommended to delete any debugging query cells if no longer needed as you go through the project.\n",
    "\n",
    "You might run into issues on the project where you are certain your code works but the output is incorrect. This may be because your collections have been corrupted. Run the following cell and uncomment the specific collections you would like to drop if you would like to remake your collections from scratch. **Be sure to re-run the Load Datasets cells below if you drop your collections so you aren't working with empty collections!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT AND RUN THIS CELL IF YOU WOULD LIKE TO REMAKE YOUR COLLECTIONS FROM SCRATCH. \n",
    "# IF YOU DROP ANY COLLECTIONS, RE-RUN THE NEXT TWO CELLS TO LOAD IN THE DATA.\n",
    "\n",
    "# review.drop()\n",
    "# business.drop()\n",
    "# user.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "The following 2 cells will load the JSON datasets into the appropriate Mongo collections. You will only need to run them once unless you drop the collections above. The second cell **may take a couple of minutes to run** if you are running it for the first time or are running it after you dropped the collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os.path\n",
    "\n",
    "if not os.path.isfile('data/yelp_academic_dataset_review.json'):\n",
    "    with zipfile.ZipFile('data/yelp_academic_dataset_review.json.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "if not os.path.isfile('data/yelp_academic_dataset_user.json'):\n",
    "    with zipfile.ZipFile('data/yelp_academic_dataset_user.json.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')\n",
    "\n",
    "if not os.path.isfile('data/yelp_academic_dataset_business.json'):\n",
    "    with zipfile.ZipFile('data/yelp_academic_dataset_business.json.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS CELL MAY TAKE AT MOST 5 MINUTES. BUT HOPEFULLY YOU WILL ONLY NEED TO RUN IT ONCE.\n",
    "if business.count_documents({}) == 0:\n",
    "    print(\"Loading business collection...\")\n",
    "    with open('data/yelp_academic_dataset_business.json', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            business.insert_one(json.loads(line))\n",
    "\n",
    "if review.count_documents({}) == 0:\n",
    "    print(\"Loading review collection...\")\n",
    "    with open('data/yelp_academic_dataset_review.json', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            review.insert_one(json.loads(line))\n",
    "            \n",
    "if user.count_documents({}) == 0:\n",
    "    print(\"Loading user collection...\")\n",
    "    with open('data/yelp_academic_dataset_user.json', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            user.insert_one(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at our collections. For the command below, replace `user` with `review` or `business` to count the number of documents in each collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user.count_documents({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's inspect our collections. Replace `business` with `review` and `user` to see the first document in each collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business.find_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a document containing a business named `Oskar Blues Taproom` when you run the command above, it means that our JSON data has successfully been imported into the collection! Now we can get started with exploring Mongo in a bit more detail.\n",
    "\n",
    "Run the cell below for grading purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Basic MQL\n",
    "\n",
    "### Question 1a\n",
    "\n",
    "In lecture, we discussed how one could find specific attributes from a JSON object using dot notation. \n",
    "\n",
    "While you can still use the dot notation in queries, PyMongo represents documents returned from Mongo queries using Python dictionaries, making it convenient to manipulate JSON using a mix of Mongo queries and array indexing. Specifically, given the result of a retrieval `find` query, you can look up the third document by appending `[2]`. Then, given this document, you can look up the field `'amount'` by appending `['amount']` etc., adding multiple square brackets as needed to \"walk down\" the JSON tree representation via `collection.find(...)[2]['amount']`.\n",
    "\n",
    "In order to get a visual output of the query results, you will need to wrap `collection.find(...)` inside `list()`, e.g. `list(collection.find(...))`.\n",
    "\n",
    "As a warmup to get you familiarized with PyMongo syntax, find the Tuesday hours for the restaurant named **Legal Sea Foods** at **100 Huntington Ave** in **Boston**. Be careful - there are many Legal Sea Foods in Boston!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_1a = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pickle.dump(result_1a, open(\"results/result_1a.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1b\n",
    "Now let's get some practice with aggregation and filtering. Our goal is to write a query that computes the average star rating for all businesses in Colorado with 30 reviews or greater. However, this won't be as easy as setting the state to CO! If we inspect this dataset more closely, we will notice that some cities are not matched up with the right states. As an example, run the query below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(business.find({\"state\": \"CA\"}).limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how cities like Portland and Atlanta, and Orlando are classified as California cities! However, the latitude and longitude is generally correct. The latitude of Colorado is between 37 and 41 **inclusive** and the longitude is between -109 and -102 **inclusive**. Now, use this to **find the average star rating** of all businesses in this range with **30 or more reviews**.\n",
    "\n",
    "Recall that in SQL, we would use a GROUP BY with the AVG aggregation function. In Mongo, we use an aggregation pipeline, comprised of multiple stages. Each stage transforms the documents in some way. Pipeline stages do not need to produce one output document for every input document. For example, some stages may generate new documents or filter out documents.\n",
    "\n",
    "**Hint 1**: as in the previous question, you may find it helpful to use the PyMongo array notation to extract the pertinent information once you have composed the right Mongo aggregation query. You are required to wrap `collection.aggregate(...)` inside `list()`, e.g. `list(collection.aggregate(...))` before indexing / visualizing the output.\n",
    "\n",
    "**Hint 2**: you can set multiple conditions for a given field within the same object, e.g. `{\"$gte\": 0, \"$lte\": 10}`. This is the recommended approach, or else you may need to worry about the ordering between the conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result_1b = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pickle.dump(result_1b, open(\"results/result_1b.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1c\n",
    "\n",
    "In this question, we will explore aggregation and grouping further. We will also make use of the `$project` operator which allows us to output documents with certain fields of our choosing. \n",
    "\n",
    "For this question, we would like to create an aggregation pipeline to find the town in each state with the highest average number of stars. **We will only consider towns with greater than or equal to 5 reviews in total across all the restaurants in that town so that the average is meaningful.** Your final output should contain exactly two fields: `city_state` which is the name of the town with the highest value of average stars in the state concatenated with a comma followed by the state initials and `averageStars` which contains the average number of stars for the corresponding town. To ensure your output is consistent with the autograder, sort in descending order by `averageStars` and break ties by sorting second on `city_state` in alphabetical order.\n",
    "\n",
    "As a concrete example, imagine that Berkeley and Austin have the highest average stars in California and Texas respectively (and both have more than or equal to 5 total reviews). If Berkeley and Austin both have an average star rating of 5.0, your final output should be:\n",
    "\n",
    "```\n",
    "{'averageStars': 5.0, 'city_state': 'Austin, TX'}\n",
    "{'averageStars': 5.0, 'city_state': 'Berkeley, CA'}\n",
    "```\n",
    "\n",
    "**NOTE:** You will provide a pipeline to `business.aggregate(...)` as your solution. Make sure that you save your pipeline to `q1c_pipeline`.\n",
    "\n",
    "**HINT:** You may find the `concat` operator helpful. See: https://docs.mongodb.com/manual/reference/operator/aggregation/concat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1c_pipeline = ...\n",
    "\n",
    "result_1c = list(business.aggregate(q1c_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "cur_test_1c = pickle.dump(result_1c, open(\"results/result_1c.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1d\n",
    "\n",
    "In class, we've described structured (rectangular) data as well as semi-structured data. We haven't quite covered unstructured data -- this is basically free-form text. Often, in semi-structured JSON you may have unstructured text data embedded within, such as the text field in the review collection.\n",
    "\n",
    "MongoDB allows us to build a so-called `text index` to retrieve the relevant document based on keywords found in text in a predefined field. This index converts our free-form text into a structure that allows us to easily look up documents by its contents. To leverage this text search capability, we must first build a text index on the text field. This has been done for you.\n",
    "\n",
    "We will then use this text index to do basic sentiment analysis and find all the restaurants we should avoid! Using the text index given, write a query to find all the reviews with \"disgusting\", \"horrible\", \"horrid\", \"gross\", \"bad\", or \"hate\". To use the text index, use the keywords `$text` and `$search` as detailed here: https://www.mongodb.com/docs/manual/core/text-search-operators/.\n",
    "\n",
    "Fill in your query into `result_1d` to count how many reviews contain either of these 6 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We create a text index here\n",
    "if 'text_text' not in review.index_information():\n",
    "    review.create_index([('text', TEXT)])\n",
    "\n",
    "result_1d = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pickle.dump(result_1d, open(\"results/result_1d.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1e\n",
    "\n",
    "Now let's learn Mongo updates, deletions, and creation. Create a new collection called `review_boolean` which is the exact same as `reviews` EXCEPT there is a new field called `to_avoid` which is the string \"true\"  if the review `text` contains the words \"disgusting\", \"horrid\", \"horrible\", \"gross\", \"bad\", or \"hate\" and the string \"false\" if not.  \n",
    "\n",
    "This is a tricky task! We have not discussed creation, updates, or insertions in great detail during lecture but luckily, Mongo uses a similar approach to SQL.\n",
    "\n",
    "*Insertions*: In order to insert into a document, you may use the functions [review_boolean.insert_one(...)](https://docs.mongodb.com/manual/reference/method/db.collection.insertOne/) or [review_boolean.insert_many(...)](https://docs.mongodb.com/manual/reference/method/db.collection.insertMany/). These functions take in a document or a list of documents and inserts them into the collection. \n",
    "\n",
    "*Updates*: In order to update a document, you may use the functions [review_boolean.update_one(...)](https://docs.mongodb.com/manual/reference/method/db.collection.updateOne/) or [review_boolean.update_many(...)](https://docs.mongodb.com/manual/reference/method/db.collection.updateMany/). These functions take in two parameters. The first specifies which documents should be modified. If the first parameter is `{}`, this indicates that all documents should be updated. However, you can put a more specific filter here if you would like. The second parameter specifies what you would like to update your field to (the [$set](https://docs.mongodb.com/manual/reference/operator/update/set/) operator may come in handy here). Recall that in our SQL model, updates are performed as `UPDATE ... SET ... WHERE ...`. In our case, the first ellipsis corresponds to `review_boolean`, the second ellipsis corresponds to the second parameter of `update_*`, and the third ellipsis corresponds to the first parameter of `update_*`.\n",
    "\n",
    "*Creation*: We handle creation of the collection for you. But in Pymongo, creation of a collection is as simple as writing `variable_name = db[collection_name]` where db is the the Pymongo database object variable you have already created.\n",
    "\n",
    "Some additional reminders and hints:\n",
    "- The empty collection `review_boolean` has already been created for you and is stored in the variable of the same name\n",
    "- A text index has been created for you. You can use a similar search approach as the last question.\n",
    "- `review.find({})` creates an iterator / list that allows you to iterate over every document in `review`. You may find this helpful in inserting individual documents into the `review_boolean` collection.\n",
    "- Do not forget that in order to pass the hidden tests, the `to_avoid` field must exist for every document in `review_boolean`! The `$exists` keyword may be helpful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_boolean = mydb[\"review_boolean\"]\n",
    "review_boolean.drop()\n",
    "\n",
    "# We create a text index here\n",
    "if 'text_text' not in review_boolean.index_information():\n",
    "    review_boolean.create_index([('text', TEXT)])\n",
    "\n",
    "# YOUR ANSWER BEGINS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "review_boolean_1e = mydb[\"review_boolean\"]\n",
    "pickle.dump(list(review_boolean_1e.find({}, {'_id': 0})), open(\"results/result_1e.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1f\n",
    "\n",
    "Now, you had a change of heart: you decide that it's unfair to label restaurants as `to_avoid` without at least giving them a chance! Remove the `to_avoid` field from the `review_boolean` collection. Calculate the `difference` between the data size of `review_boolean` with the `to_avoid` field and without it. The code for making this calculation is provided but it is up to you to actually remove the field.\n",
    "\n",
    "*Deletions*: Deletions in Mongo make use of the `review_boolean.update_one(...)` or `review_boolean.update_many(...)` functionality discussed in Question 1e. However, this time, instead of using the `$set` operator which allows for the creation of new fields, we will use the [$unset](https://docs.mongodb.com/manual/reference/operator/update/unset/) operator which deletes them! Very tidy!\n",
    "\n",
    "**Before running the next cell, make sure to re-run your cell for 1e so you don't get a difference of 0!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_avoid = mydb.command(\"collstats\", \"review_boolean\")['size']\n",
    "\n",
    "# YOUR ANSWER BEGINS HERE\n",
    "# END\n",
    "\n",
    "without_avoid = mydb.command(\"collstats\", \"review_boolean\")['size']\n",
    "difference = with_avoid - without_avoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pickle.dump(difference, open(\"results/result_1f.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: JSON and Relational Models\n",
    "\n",
    "### Question 2a\n",
    "\n",
    "Now we have a good idea of how to do retrieval, aggregation, and updates in Mongo. But we haven't talked about why we\n",
    "would want to use Mongo to store JSON! In order to explore this, let's take another look at the `business`\n",
    "collection. We will look at the first two entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(business.find({}).limit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "What are **two** pros of storing this data in MongoDB with JSON over a relational database management system such as Postgres?\n",
    "Please reference specific examples from the `business` collection to back up your claims. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "manual: true\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "It seems like MongoDB is getting all the love when it comes to JSON support! However, modern iterations of relational databases\n",
    "such as Postgres 9.3+ also have [excellent JSON functionality](https://www.postgresql.org/docs/9.3/functions-json.html) as we will soon explore in this task. First, let's set up a\n",
    "bit of scaffolding. The following cell will import the `yelp_academic_dataset_review.json` data into a table called `reviews` in Postgres yelp database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext sql\n",
    "%sql postgresql://jovyan@127.0.0.1:5432/postgres\n",
    "\n",
    "!psql -h localhost -c 'DROP DATABASE IF EXISTS yelp'\n",
    "!psql -h localhost -c 'CREATE DATABASE yelp'\n",
    "!psql -h localhost -d yelp -c 'DROP TABLE IF EXISTS reviews'\n",
    "!psql -h localhost -d yelp -c 'CREATE TABLE reviews(data TEXT);'\n",
    "!cat data/yelp_academic_dataset_review.json | psql -h localhost -d yelp -c \"COPY reviews (data) FROM STDIN;\"\n",
    "%sql \\l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cell to connect to the Postgres yelp database. There should be no errors after running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql postgresql://jovyan@127.0.0.1:5432/yelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to observe how this new `reviews` table looks. Note that the `data` column is stored as TEXT and not as JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM reviews LIMIT 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the reviews table consists of one column named `data`. This column contains all the JSON documents in the \n",
    "reviews collection *in text format*. Use [Postgres' JSON functions](https://www.postgresql.org/docs/9.3/functions-json.html) to write a query that converts the JSON fields into their own `TEXT` columns (hint: one of the operators in Table 9-40 may be useful). To be more concrete, your query should contain 8 columns in this particular order: `review_id`, `user_id`, `business_id`, `stars`, `useful`, `funny`, `cool`, and `text`. Each row should correspond to one JSON document. Some skeleton code (that does the mundane work of converting data to JSON properly) is provided to you - you will only need to fill in the SELECT clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql result_2b <<\n",
    "...\n",
    "FROM (SELECT CAST(regexp_replace(data, E'[\\\\n\\\\r]+', '','g') AS JSON) AS values FROM reviews) b\n",
    "ORDER BY review_id\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "result_2b.DataFrame().to_csv('results/result_2b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2c\n",
    "\n",
    "One important aspect of data engineering that we have not referred to yet are joins. We saw, through the use of indices, selection/projection pushdown, and various physical implementations (as well as orderings), joins could be done quite efficiently in relational SQL based databases. How do joins fare in Mongo where the data stored is inherently semistructured? Let's investigate! For this question, we have provided you access to the tables `business_complete` and `review_complete` which contain the business and review collections in relational form as described in 2b (the columns of the relations\n",
    "are fields in the JSON document). Each relation has its respective id (`business_id` or `review_id`) column as its primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "!psql -h localhost -d yelp -c 'DROP TABLE IF EXISTS business_complete'\n",
    "!psql -h localhost -d yelp -c 'CREATE TABLE business_complete(business_id TEXT PRIMARY KEY, name TEXT, address TEXT, city TEXT, state TEXT, postal_code TEXT, latitude TEXT,longitude TEXT, stars TEXT, review_count TEXT, is_open TEXT, attributes TEXT, categories TEXT, hours TEXT);'\n",
    "!psql -h localhost -d yelp -c 'DROP TABLE IF EXISTS review_complete'\n",
    "!psql -h localhost -d yelp -c 'CREATE TABLE review_complete(review_id TEXT PRIMARY KEY, user_id TEXT, business_id TEXT, stars TEXT, useful TEXT, funny TEXT, cool TEXT,text TEXT);'\n",
    "!cat data/business.csv | psql -h localhost -d yelp -c \"COPY business_complete (business_id,name,address,city,state,postal_code,latitude,longitude,stars,review_count,is_open,attributes,categories,hours) FROM STDIN CSV HEADER;\"\n",
    "!cat data/review.csv | psql -h localhost -d yelp -c \"COPY review_complete (review_id, user_id, business_id, stars, useful, funny, cool, text) FROM STDIN CSV HEADER;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how `review_complete` looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM review_complete LIMIT 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this current moment in time, Mongo only supports left joins (via the lookup aggregation stage). This is what we will compare against SQL.\n",
    "\n",
    "Let's start by writing a SQL query that displays all the reviews along with their associated business information. You should perform a **left join** between the `review_complete` table and the `business_complete` table, and you may project all columns. Keep a mental note of the **execution time** that you see in the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_2c_str = ...\n",
    "!psql -h localhost -d yelp -c \"explain analyze $result_2c_str\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform the equivalent left join in Mongo between `review` and `business`. Feel free to refer to the `$lookup` documentation: https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/.\n",
    "\n",
    "**NOTE:** You will provide a single-stage pipeline to `review.aggregate(...)` as your solution. Make sure that you save your pipeline to `q2c_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first create an index on business_id in the business collection\n",
    "business.create_index('business_id', unique=True)\n",
    "\n",
    "q2c_pipeline = ...\n",
    "\n",
    "result_2c = list(review.aggregate(q2c_pipeline))[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "cur_test_2c = pickle.dump(result_2c, open(\"results/result_2c.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to examine the query plan for the Mongo query that you just wrote. Again, make a mental note of the execution time that you see (you can find the value corresponding to the key `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydb.command('explain', {'aggregate': 'review', 'pipeline': q2c_pipeline, 'cursor': {}}, verbosity='executionStats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2d\n",
    "\n",
    "In the last question, you performed equivalent left joins in both Postgres and Mongo, and examined their query plans. Which join was faster? What gives that database system you chose an advantage over the other?\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2d\n",
    "manual: true\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Question 3: Dataframes / Pandas\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "So far, we've talked about NoSQL / document databases like Mongo and relational databases like Postgres. Now, we will explore data transformation with a different data model: dataframes. Dataframes are similar to relations with some differences as we will dive into here. To that end, we will use Pandas which is a Python package that allows you to work with dataframes. Pandas is widely adopted by data scientists for data loading, wrangling, cleaning, and analysis. To start, let us export our MongoDB collections into Pandas using a function called `json_normalize`. We need to truncate\n",
    "`business` before we can use it to meet the memory constraints set by Jupyter. The variable `business_trunc` will contain the reference the truncated business collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_trunc = mydb[\"business_trunc\"]\n",
    "count = 0\n",
    "if business_trunc.count_documents({}) != 1000:\n",
    "    for document in business.find({}):\n",
    "        count += 1\n",
    "        business_trunc.insert_one(document)\n",
    "        if count == 1000:\n",
    "            break\n",
    "\n",
    "business_cursor = business_trunc.find({})\n",
    "review_cursor = mydb[\"reviews\"].find({})\n",
    "user_cursor = mydb[\"users\"].find({})\n",
    "\n",
    "# Load the collections into Pandas. \n",
    "from pandas import json_normalize\n",
    "user_df = json_normalize(user_cursor)\n",
    "review_df = json_normalize(review_cursor)\n",
    "business_df = json_normalize(business_cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of Question 3, please use the 3 dataframes we just created: `user_df`, `review_df`, and `business_df`. Let's take a look at the first 5 rows of `business_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "What do you notice about how the columns of `business_df` are constructed? Compare and contrast this dataframe representation with the document representation we saw with Mongo.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3a\n",
    "manual: true\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "In the previous question, we talked about how Mongo and Postgres approach joins. Pandas is also capable of performing joins using the [merge()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) function! For this task, perform a inner join on `business_df` with itself on `stars`. The final dataframe should be saved to a variable called `result_3b` and should only contain 3 columns in this particular order: the name of the first restaurant, the name of the second restaurant, and the number of the stars. The column names can be arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_3b = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "result_3b.columns = ['first', 'second', 'stars']\n",
    "result_3b.sort_values(['first', 'second', 'stars'])[:50].to_csv('results/result_3b.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3c\n",
    "\n",
    "Due to the nested representation of the data, there are a lot of missing fields with NaN values in the Pandas dataframes as you may have noticed in 3a. Construct a dataframe `missing_value_df` with two columns: `column_name` and `percent_missing`. `percent_missing` should be the percentage of NaN values in the corresponding column.\n",
    "\n",
    "**Hint:** use Pandas' [isnull()](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html) function followed by sum()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_value_df = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "missing_value_df.to_csv('results/result_3c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3d\n",
    "\n",
    "Plot a histogram distribution of the percentage of NaN values across all columns (via Pandas [hist()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html) function). Don't worry about putting titles / making it look nice - we won't be grading the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examine the histogram that you just plotted. How many columns are 90%+ NaN? Input your answer into `result_q3d` as an integer (e.g. if your answer is 6, then `result_q3d = 6`)\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3d\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_q3d = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3e\n",
    "\n",
    "Let us now alter `business_df` to exclude the columns with more than 80%+ null values (keep columns with 80% null values or less). This likely means the corresponding attributes are not an important factor for most businesses so we can get rid of them in our `business_df`. Create a new dataframe called `important_attribute_business_df` which only contains these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_attribute_business_df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "important_attribute_business_df.to_csv('results/result_3e.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 3f\n",
    "\n",
    "At this point, you have had experience with manipulating data on Mongo, Postgres, and Pandas. In this question, we will provide 3 scenarios and using the lessons you've learned so far, please specify which of the three (Mongo, Postgres, or Pandas) would work best for this specific use case.\n",
    "\n",
    "1. You are doing a data journalism piece on college sports. You collect a list of colleges and for each collegiate sport program within that college, you find the budget assigned for that program. You have a choice between the following:\n",
    "\n",
    "    A) Representing this data in JSON (e.g. \n",
    "    ```\n",
    "    {\n",
    "        \"UC Berkeley\": {\n",
    "            \"football\": \"10000000\", \n",
    "            \"wrestling\": \"344582\", \n",
    "            ...}\n",
    "    }\n",
    "    ```\n",
    "    ) and importing into Mongo.\n",
    "    \n",
    "    B) Representing this data as a schema in Postgres where the columns are the names of the sports.\n",
    "    \n",
    "    C) Representing this data as a dataframe in Pandas where the columns are the names of the sports.\n",
    "\n",
    "You would like to find the aggregate of budgets across different sports (average, sum, median, mode). What would be the best option for storing this data?\n",
    "\n",
    "**NOTE**: Your answer should look like `q3fi_str = ['A']` or `q3fi_str = ['B']` or `q3fi_str = ['C']`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3fi\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3fi_str = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3fi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "2. You would now like to investigate what effect does budget have on student-athlete scholarships. After doing some research, you find a dataset that contains a list of every single athlete at every single college and their sport and scholarship levels (this is a massive 10GB+ dataset with millions of rows). You find another dataset that contains a list of colleges, their sports programs, and the program budget. This is another massive dataset with hundreds of thousands of rows. You would like to perform an inner join between the two datasets on school and program so you can view each student-athlete's scholarship with their sport's budget. You have a choice between the following:\n",
    "\n",
    "    A) Representing each dataset in JSON (e.g. \n",
    "    ```\n",
    "    {\"athletes\": [\n",
    "        {\"Chase Garbers\": {\n",
    "            \"school\": \"UC Berkeley\", \n",
    "            \"scholarship\": \"full\", \n",
    "            \"sport\": \"football\", \n",
    "            ...\n",
    "            }\n",
    "        }, \n",
    "        ...\n",
    "    ]}\n",
    "    ```\n",
    "    and \n",
    "    ```\n",
    "    {\"schools\": [\n",
    "        {\"UC Berkeley\": {\n",
    "            \"football\": {\n",
    "                \"budget\": \"10000000\"\n",
    "             }, \n",
    "             ...\n",
    "             }\n",
    "        }, \n",
    "        ...\n",
    "     ]}\n",
    "     ```\n",
    "    ), importing into Mongo, and doing a join there.\n",
    "    \n",
    "    B) Representing this data as 2 schemas in Postgres where the columns for the first schema are \n",
    "    [`student_name`, `school`, `sport`, `scholarship`] and for the second [`school`, `sport`, `budget`].\n",
    "    \n",
    "    C) Representing this data as 2 dataframes in Pandas with the same columns as Postgres.\n",
    "\n",
    "What would be the best option for storing this data?\n",
    "\n",
    "**NOTE**: Your answer should look like `q3fii_str = ['A']` or `q3fii_str = ['B']` or `q3fii_str = ['C']` or `q3fii_str = ['D']`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3fii\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3fii_str = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3fii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "3. Finally, you are ready to start writing your article! You decide to focus on just the data from UC Berkeley. You have access to a dataset of just UC Berkeley athletes along with their sports and scholarship levels. The scholarship level data was improperly cleaned: some scholarships are recorded as strings \"full\", \"half\", or \"none\" and some are recorded as integer percentages 0-100. You would like to provide this data to your readers in a format that is susceptible to easy visualizations: e.g. graphs that show how many athletes have a full vs. half vs. no scholarship, which sports have the highest percentages of athletes with full scholarships etc. What is the best way to store this data for this purpose?\n",
    "\n",
    "    A) Represent the dataset in JSON e.g.\n",
    "    ```\n",
    "    {\"athletes\": [\n",
    "        {\n",
    "           \"Chase Garbers\": {\n",
    "             \"scholarship\": \"full\", \n",
    "             \"sport\": \"football\"\n",
    "           }\n",
    "        },\n",
    "        {\n",
    "            \"Danielle Vosk\": {\n",
    "              \"scholarship\": 25,\n",
    "              \"sport\": \"basketball\"\n",
    "            }\n",
    "        },\n",
    "        ...\n",
    "        ]\n",
    "    }\n",
    "    ```\n",
    "    B) Represent this data as a schema in Postgres where the columns are [`student_name`, `sport`, `scholarship`]\n",
    "    \n",
    "    C) Represent this data as a dataframe in Pandas with the same columns as Postgres.\n",
    "    \n",
    "**NOTE**: Your answer should look like `q3fiii_str = ['A']` or `q3fiii_str = ['B']` or `q3fiii_str = ['C']` or `q3fiii_str = ['D']`\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3fiii\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3fiii_str = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3fiii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Messy JSON\n",
    "\n",
    "Many of the queries you've seen or written thus far were relatively reliable: aggregating and collecting over fields\n",
    "that you know exist for sure. But the nature of Mongo documents is that they are inherently flexible and semi-structured. Not every document will share every single field! In this question, we will explore how Mongo handles these use cases using the `business` collection.\n",
    "\n",
    "### Question 4a\n",
    "\n",
    "Imagine you are in charge of managing your family reunion. You would like to book a private room at a restaurant.\n",
    "However, you would also like to optimize for chaos. You notice that there is an attribute called `RestaurantsGoodForGroups`. You would like to write a query that returns all restaurants that **do not** have the `RestaurantsGoodForGroups` attribute so that the trajectory of the reunion is determined by fate (hint: search up the `$exists` keyword). Your output for the autograder will be the integer number of restaurants that do not have the `RestaurantsGoodForGroups` attribute stored in `q4a_str`. \n",
    "\n",
    "**NOTE: You would like this list to consist solely of restaurants. This means that the business must have `Restaurants` in the `categories` field. You may perform a similar text search as question 1d.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following text index may be useful!\n",
    "if 'categories_text' not in business.index_information():\n",
    "    business.create_index([('categories', TEXT)])\n",
    "\n",
    "...\n",
    "q4a_str = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many restaurants do not have the `RestaurantsGoodForGroups` attribute? You may either enter input this is a function with respect to your query or hardcode in either the String or the numeric version of the answer you computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pickle.dump(q4a_str, open(\"results/result_4a.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b\n",
    "\n",
    "Your relatives inform you that they would like to be at the restaurant when it opens to beat the crowds. Furthermore, after sending\n",
    "a when2meet, most of your relatives would prefer for the meal to be on a Friday and the start time of the meal to be \n",
    "between 5-6:59PM (17:00-18:59). Find the number of restaurants that open on Fridays between 17:00-18:59 and store this in a variable labelled `q4b_str`. As a reminder, in order for a business to be a restaurant, it must have `Restaurant` in its categories. Be aware that `hours` can either be an array or `None`!\n",
    "\n",
    "**HINT**: It will be advantageous to use the `aggregate()` Pymongo function along with the `$set` and `$match` stage operators. You may also want to use the `$split` operator (similar to Python's string `split()` function) to parse out the Friday hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "q4b_str = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pickle.dump(q4b_str, open(\"results/result_4b.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4c\n",
    "\n",
    "Some members of your family are vegetarian so you would like to only eat at restaurants with the Vegetarian category. \n",
    "However, the `categories` are stored as a single string! You would like to make it easy to access Vegetarian as a separate field. Write a query that does the following: for every category in `categories`, add a new document that contains the category as a field with the value `'true'`, the `ObjectId` for the previous document (labelled `_id`), and the name of the business (labeled `name`).\n",
    "\n",
    "For example, a document \n",
    "```\n",
    "{\n",
    "    \"_id\": ObjectId('606ffb0123cf2e5079dbd91f'), \n",
    "    \"name\": \"Wendy's\", \n",
    "     ..., \n",
    "     categories\" : \"Salad, Vegetarian\"\n",
    "} \n",
    "```\n",
    "would become \n",
    "```\n",
    "{\n",
    "    “Salad”: \"true\", \n",
    "    \"_id\": ObjectId('606ffb0123cf2e5079dbd91f'), \n",
    "    \"name\": \"Wendy's\"\n",
    "}\n",
    "```\n",
    "and \n",
    "```\n",
    "{\n",
    "    “Vegetarian”: \"true\", \n",
    "    \"_id\": ObjectId('606ffb0123cf2e5079dbd91f'), \n",
    "    \"name\": \"Wendy's\"\n",
    "}\n",
    "```\n",
    "\n",
    "Save your pipeline to a variable called `q4c_pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "q4c_pipeline = ...\n",
    "\n",
    "result_4c = list(business.aggregate(q4c_pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pipeline_for_test_4c = q4c_pipeline[:]\n",
    "pipeline_for_test_4c.extend([{\"$match\": {\"_id\": ObjectId('635b5ed2b01389c24d31ce23')}}, {\"$project\": {\"_id\": 0}}])\n",
    "cur_test_4c = business.aggregate(pipeline_for_test_4c)\n",
    "pickle.dump(list(cur_test_4c), open(\"results/result_4c.p\", \"wb\"))\n",
    "\n",
    "pipeline_for_test_4c_2 = q4c_pipeline[:]\n",
    "pipeline_for_test_4c_2.extend([{\"$match\": {\"_id\": ObjectId('635b5ecab01389c24d319af5')}}, {\"$project\": {\"_id\": 0}}])\n",
    "cur_test_4c_2 = business.aggregate(pipeline_for_test_4c_2)\n",
    "pickle.dump(list(cur_test_4c_2), open(\"results/result_4c_2.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4d\n",
    "This change in representation has made it super easy to view all the vegetarian restaurants and count them without the use of an index since we can now simply filter by whether or not 'Vegetarian' is a field in our document! We have provided some code here to count how many vegetarian restaurants are in our dataset. Simply provide the integer count to get a point for this question :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "q4d_pipeline = q4c_pipeline[:]\n",
    "q4d_pipeline.append({\"$match\": {\"Vegetarian\": 'true'}})\n",
    "result_4d = list(business.aggregate(q4d_pipeline))\n",
    "\n",
    "veg_count = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete/edit this cell\n",
    "pickle.dump(veg_count, open(\"results/result_4d.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You have finished Project 4.\n",
    "\n",
    "Run the following cell to zip and download the results of your queries. You will also need to run the export cell at the end of the notebook.\n",
    "\n",
    "**For submission on Gradescope, you will need to submit BOTH the proj4.zip file genreated by the export cell and the results.zip file generated by the following cell.**\n",
    "\n",
    "**Common submission issues:** You MUST submit the generated zip files (not folders) to the autograder. However, Safari is known to automatically unzip files upon downloading. You can fix this by going into Safari preferences, and deselect the box with the text \"Open safe files after downloading\" under the \"General\" tab. If you experience issues with downloading via clicking on the link, you can also navigate to the project 4 directory within JupyterHub (remove `proj4.ipynb` from the url), and manually download the generated zip files. Please post on Ed if you encounter any other submission issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, FileLink\n",
    "\n",
    "!zip -r results.zip results\n",
    "results_file = FileLink('./results.zip', result_html_prefix=\"Click here to download: \")\n",
    "display(results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
